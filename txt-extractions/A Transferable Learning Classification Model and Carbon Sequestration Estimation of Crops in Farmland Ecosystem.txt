Citation: Wang, L.; Bai, Y.; Wang, J.;Qin, F.; Liu, C.; Zhou, Z.; Jiao, X. ATransferable Learning ClassiﬁcationModel and Carbon SequestrationEstimation of Crops in FarmlandEcosystem. Remote Sens. 2022 ,14,5216. https://doi.org/10.3390/rs14205216Academic Editors: StanisławLewi ´ nski and Wojciech DrzewieckiReceived: 24 August 2022Accepted: 16 October 2022Published: 18 October 2022Publisher’s Note: MDPI stays neutralwith regard to jurisdictional claims inpublished maps and institutional afﬁl-iations.Copyright: © 2022 by the authors.Licensee MDPI, Basel, Switzerland.This article is an open access articledistributed under the terms andconditions of the Creative CommonsAttribution (CC BY) license (https://creativecommons.org/licenses/by/4.0/).remote sensing  ArticleA Transferable Learning Classiﬁcation Model and CarbonSequestration Estimation of Crops in Farmland EcosystemLijun Wang1,2,3, Yang Bai1,2,3,*, Jiayao Wang1,2,4, Fen Qin1,2,4,5, Chun Liu1,2,3, Zheng Zhou1,2,3and Xiaohao Jiao2,31Henan Industrial Technology Academy of Spatio-Temporal Big Data, Henan University,Kaifeng 475004, China2College of Geography and Environmental Science, Henan University, Kaifeng 475004, China3Key Laboratory of Geospatial Technology for the Middle and Lower Yellow River Regions,Ministry of Education, Henan University, Kaifeng 475004, China4Henan Technology Innovation Center of Spatial-Temporal Big Data, Henan University, Kaifeng 475004, China5Key Research Institute of Yellow River Civilization and Sustainable Development, Ministry of Education,Henan University, Kaifeng 475004, China*Correspondence: daisy.baiyang@henu.edu.cnAbstract: Frequent agricultural activities in farmland ecosystems bring challenges to crop informationextraction from remote sensing (RS) imagery. The accurate spatiotemporal information of crops servesfor regional decision support and ecological assessment, such as disaster monitoring and carbonsequestration. Most traditional machine learning algorithms are not appropriate for predictionclassiﬁcation due to the lack of historical ground samples and poor model transfer capabilities.Therefore, a transferable learning model including spatiotemporal capability was developed basedon the UNet++ model by integrating feature fusion and upsampling of small samples for Sentinel-2Aimagery. Classiﬁcation experiments were conducted for 10 categories from 2019 to 2021 in XinxiangCity, Henan Province. The feature fusion and upsampling methods improved the performance of theUNet++ model, showing lower joint loss and higher mean intersection over union (mIoU) values.Compared with the UNet, DeepLab V3+, and the pyramid scene parsing network (PSPNet), theimproved UNet++ model exhibits the best performance, with a joint loss of 0.432 and a mIoU of0.871. Moreover, the overall accuracy and macro F 1values of prediction classiﬁcation results basedon the UNet++ model are higher than 83% and 58%, respectively. Based on the reclassiﬁcation rules,about 3.48% of the farmland was damaged in 2021 due to continuous precipitation. The carbonsequestration of ﬁve crops (including corn, peanuts, soybean, rice, and other crops) is estimated, witha total carbon sequestration of 2460.56, 2549.16, and 1814.07 thousand tons in 2019, 2020, and 2021,respectively. The classiﬁcation accuracy indicates that the improved model exhibits a better featureextraction and transferable learning capability in complex agricultural areas. This study provides astrategy for RS semantic segmentation and carbon sequestration estimation of crops based on a deeplearning network.Keywords: transferable learning; crop segmentation and classiﬁcation; Sentinel-2A; UNet++; car-bon sequestration1. IntroductionAs a part of the terrestrial ecosystem, the farmland ecosystem plays an important rolein global climate change, food production, and ecological assessment [ 1]. As an increasingnumber of satellite observations become available, there is an urgent need to obtain accu-rate crop maps from the massive images and assess the signiﬁcance of each crop type infarmland ecosystems. In recent years, several studies have been carried out on crop moni-toring and carbon sequestration in different ecosystems. However, these studies mainlyfocused on the forest ecosystem and grassland ecosystem [ 2–4]. Due to the strong humanRemote Sens. 2022 ,14, 5216. https://doi.org/10.3390/rs14205216 https://www.mdpi.com/journal/remotesensingRemote Sens. 2022 ,14, 5216 2 of 19factors, crops in farmland have seasonal and complex characteristics, leading to morechallenges in crop automatic classiﬁcation and carbon sequestration estimation. Therefore,a method must be established to extract accurate crop information using remote sensing(RS) technology in order to estimate crop area, disaster loss, and carbon sequestration.Farmland vegetation is an essential part of the carbon cycle of the terrestrial ecosystem,and the crops play different roles in carbon sequestration [ 5]. As a developing agriculturalcountry with a large population, cropland accounts for about 12.5% of the total area ofChina. It is necessary to pay more attention to monitoring the crop planting area andits impact on the carbon cycle. The emerging satellite technologies with diverse sensorsprovide reliable large-scale land use and crop observation [ 6]. In particular, the Sentinel-2A (S-2A) satellite developed by the European Space Agency provides 13-band opticalimagery with a high spatial resolution of 10 m. The S-2A images have been widely usedin global agricultural monitoring, such as crop yield estimation, disaster monitoring,and environmental assessment. In addition, the Google Earth Engine (GEE) providesan interactive platform for feature acquisition and geospatial algorithm developmentfor satellite imagery [ 7,8]. Several studies show that it is convenient to obtain multiplefeature images from GEE, which are essential for crop classiﬁcation in complex agriculturalareas [ 7,9]. Therefore, combining spectral bands, derived features (including vegetableindices and texture features), and crop phenological information, has become an importantmethod to overcome the problems of within-class diversity and between-class similarityand to obtain high-accuracy crop information.Due to the seasonality of crop planting, the acquisition of ground samples and imageryoften has to be completed in a short time. On the one hand, it is difﬁcult to obtain time-series optical images during the crop growth period because of climatic factors. On theother hand, traditional machine learning methods, such as support vector machine (SVM),random forest (RF), and object-oriented classiﬁcation methods, rely on a large number ofhigh-quality ground sample data sets [ 10]. However, these sample data sets would lose theireffectiveness due to crop seasonality limitations across years. Furthermore, the pixel-basedclassiﬁcation results struggle to meet the precise application requirements due to the “saltand pepper phenomenon” [ 8]. For the object-oriented classiﬁcation method, it is difﬁcult todetermine the optimal segmentation scale due to the image resolution, crop complexity, andﬁeld fragmentation, resulting in over-segmentation and under-segmentation problems [ 11].Despite the success of traditional machine learning in earth science, the above problemsand limitations have hampered the model performance and classiﬁcation accuracy [ 12].To alleviate the dependence and workload related to ground investigations, it is essentialto use the transferable learning capability of classiﬁcation models and to complete theautomatic extraction of crop information from RS images based on few label data sets.Deep learning has become a semantic segmentation and target detection tool for solv-ing many challenging problems in computer vision [ 9,12]. As data-driven deep neuralnetworks, deep learning networks usually outperformed shallow classiﬁers due to thehierarchical feature transformation capability from multi-feature RS imagery. The 2012ImageNet classiﬁcation challenge and massive label data sets have been crucial in demon-strating the effectiveness of deep convolutional neural networks (CNNs) [ 13]. However,it is difﬁcult to obtain a large number of spatiotemporal crop data sets for RS images.The end-to-end transferable learning capacity reduced manual feature engineering andimproved model performance. Many studies have shown that CNNs and fully convo-lutional networks (FCNs) are important network architecture for RS image and naturalscene semantic segmentation [ 14,15]. Furthermore, FCNs introduced the encoder–decoderparadigm, which transforms the feature map size to the original image size and overcomesthe limitations of losing detailed information in the down-sampling and the ﬁxed sizeof the input data set [ 16]. Therefore, the development of FCN facilitates the transferablemodel performance and high-accuracy crop mapping in the large-scale region across years.To obtain high-accuracy crop maps, current studies mainly focus on feature fusionand FCN-based semantic segmentation methods. Li et al. [ 17] proposed an improvedRemote Sens. 2022 ,14, 5216 3 of 19deep learning method to classify crops of corn and soybean from RS time-series imagesand achieved the classiﬁcation results with a Kappa coefﬁcient of 0.79 and an overallaccuracy of 0.86. Giannopoulos et al. [ 18] extended a deep learning model based on UNetarchitecture to extract information from Landsat-8 images and achieved higher accuracythan low-order deep learning models. Yang et al. [ 19] used different semantic segmen-tation models, including temporal feature-based segmentation model, long short-termmemory (LSTM), and UNet, to accomplish rice mapping based on time-series SAR im-ages. Wang et al. [ 20] proposed a two-stage model that fused DeepLab V3+ and UNetfor cucumber leaf disease severity classiﬁcation with a segmentation accuracy of 93.27%.The above-mentioned studies have achieved high classiﬁcation accuracy and provided areference for image segmentation and classiﬁcation of major crops using remote sensingimages. However, applying the semantic segmentation models of the studies in com-plex agricultural areas with imbalanced samples still needs further testing. Moreover,Zhou et al. [ 21] proved that the UNet++ model had more advantages in feature map-generating strategy and image semantic segmentation with few training sample data sets.Wang et al. [ 9] used the improved UNet++ architecture to classify 10 categories fromSentinel-2 imagery (including 17 bands of spectral, vegetation indices, and texture features)in 3 years, and indicated that the UNet++ achieved higher segmentation and classiﬁcationaccuracy than UNet and DeepLab V3+. These studies provided methodological supportfor feature fusion and agricultural information extraction of Sentinel-2 imagery. However,time series prediction classiﬁcation results of recent research still need to be further appliedand analyzed in farmland ecosystems, such as in disaster assessment and crop carbon se-questration. Therefore, it is essential to establish a regional and transferable-learning modelwith few training data sets for RS crop classiﬁcation and carbon sequestration estimationduring the critical crop growth stage.Chinese agricultural land has experienced dramatic changes in crop area, croppingsystem, and planting structure optimization in recent decades [ 22]. These changes cansubstantially affect crop carbon sequestration in the farmland. The widespread and scat-tered smallholders in China have a profound impact on agricultural production in responseto climate change. Tang et al. [ 23] evaluated the contribution and estimation error of thefarmland ecosystem to carbon sequestration in China’s terrestrial ecosystem. The uncer-tainties of carbon sequestration estimation can be reduced by collecting the ﬁner crop areaand statistical data in farmland ecosystem. In terms of agro-ecosystem services, vegeta-tion indices and time-series production were used to evaluate and predict the impact ofcurrent vegetation cover on the farmland ecosystem [ 24,25], or to quantify the spatiotem-poral changes, including the assessment of carbon sources/sinks and soil erosion [ 26].Zhang et al. [27] provided a novel perspective on LCC (land cover change)-induced grossprimary production (GPP) changes and concluded that the LCC-induced reduction in GPPwas partially offset by increases in cropland using the GEE platform. Wang et al. [ 28]showed that the changes in crop planting area can substantially affect greenhouse gasemissions, and that farmland in China was a carbon source due to a large amount of CH4emission in paddy lands. Such a capability to collect timely and high-accuracy crop infor-mation from multispectral imagery using the transferable-learning model and to produceaccurate crop maps is crucial to assess the farmland ecosystem services.In this study , we pay more attention to the automatic classification of crop informationfrom S-2A imagery in complex agricultural areas. As an agricultural region in China, XinxiangCity of Henan Province has a representative topography and diversified crop types. Thisstudy aims to (1) evaluate the influence of feature-selection schemes on the UNet++ model;(2) evaluate the performance of different deep learning models; (3) compare prediction classi-fication accuracies of different models based on overall accuracy , user’s accuracy , producer’saccuracy , and F 1scores; and (4) complete crop mapping and carbon sequestration estimationin farmland ecosystem. Overall, this work aims to offer an improved deep learning procedureas applied to RS in crop monitoring and carbon sequestration estimation.Remote Sens. 2022 ,14, 5216 4 of 192. Study Sites and Data2.1. Study SitesThis study focused on crop mapping in Xinxiang City of Henan Province, China(Figure 1), with a total area of about 8251.08 km2. The mountains are mainly distributedin the northern part. The crop types in the plain area are diverse, mainly including corn,soybean, peanut, rice, other crops (OTH, including vegetables, tobacco, and Chinese herbalmedicine), non-cultivated land (NCL), and greenhouses (GH). Other categories includeurban (URB, including residential areas, roads, and other construction), forest land (FL,including nursery forest planted on farmland, green areas around cities and villages, andmountain areas), and water (WAT, including the rivers, ditches, ponds, and inundatedareas). The crops are sown in early June and harvested in mid-September, with a growthperiod of about 110 d. The mean annual precipitation of the study area is about 581 mmand concentrated from mid-July to late August. However, the precipitation in 2021 reaches1377.6 mm, which limits the acquisition of time-series optical satellite images. Four sites(A, B, C, and D) in 2019 and 2020 were chosen as the source of training data sets for thedeep learning model, each covering ~20 20 km. In addition, ﬁve sample plots (S1–S5),six validation sample plots (VS1–VS6), and some validation sample points are selected andinvestigated for image labeling and classiﬁcation accuracy evaluation.Remote Sens. 2022, 14, x FOR PEER REVIEW   4 of 20   accuracy,  user’s accuracy,  producer’s  accuracy,  and F1 scores; and (4) complete  crop map‐ping and carbon sequestration  estimation  in farmland  ecosystem.  Overall, this work aims to offer an improved  deep learning procedure  as applied to RS in crop monitoring  and carbon sequestration  estimation.  2. Study Sites and Data 2.1. Study Sites This study focused on crop mapping  in Xinxiang  City of Henan Province,  China (Fig‐ure 1), with a total area of about 8251.08 km2. The mountains  are mainly distributed  in the northern  part. The crop types in the plain area are diverse, mainly including  corn, soy‐bean, peanut, rice, other crops (OTH, including  vegetables,  tobacco, and Chinese herbal medicine),  non‐cultivated  land (NCL), and greenhouses  (GH). Other categories  include urban (URB, including  residential  areas, roads, and other construction),  forest land (FL, including  nursery forest planted on farmland,  green areas around cities and villages, and mountain  areas), and water (WAT, including  the rivers, ditches, ponds, and inundated  areas). The crops are sown in early June and harvested  in mid‐September,  with a growth period of about 110 d. The mean annual precipitation  of the study area is about 581 mm and concentrated  from mid‐July to late August. However,  the precipitation  in 2021 reaches 1377.6 mm, which limits the acquisition  of time‐series optical satellite images. Four sites (A, B, C, and D) in 2019 and 2020 were chosen as the source of training data sets for the deep learning model, each covering  ~20 × 20 km. In addition,  five sample plots (S1–S5), six validation  sample plots (VS1–VS6),  and some validation  sample points are selected and investigated  for image labeling and classification  accuracy  evaluation.   Figure 1. Geography  of Xinxiang  City on the northern  plain of Henan Province,  China. 2.2. Sentinel‐2A Data Due to the low vegetation  cover and soil spectral effects in the early stage, high qual‐ity and cloudless  images with similar dates were selected in the late growth stage in early September  of 2019, 2020, and 2021. On 4 September  2019, 5 September  2020, and 9 Sep‐tember 2021, we collected  five satellite images covering  the whole area, with orbital num‐bers of T49SGU,  T50SGV,  T50SKD,  T50SKE, and T50SLE, respectively.  Sentinel Level‐2A atmospheric  bottom reflectance  data follow the U.S. Military Grid Reference  System (US‐MGRS) to set the satellite orbit number (such as T50SGV)  and have been preprocessed  in GEE [8]. Hence, we can quickly calculate  and compose  S‐2A images with different  features Figure 1. Geography of Xinxiang City on the northern plain of Henan Province, China.Remote Sens. 2022 ,14, 5216 5 of 192.2. Sentinel-2A DataDue to the low vegetation cover and soil spectral effects in the early stage, highquality and cloudless images with similar dates were selected in the late growth stage inearly September of 2019, 2020, and 2021. On 4 September 2019, 5 September 2020, and9 September 2021, we collected ﬁve satellite images covering the whole area, with orbitalnumbers of T49SGU, T50SGV , T50SKD, T50SKE, and T50SLE, respectively. Sentinel Level-2A atmospheric bottom reﬂectance data follow the U.S. Military Grid Reference System(US-MGRS) to set the satellite orbit number (such as T50SGV) and have been preprocessedin GEE [ 8]. Hence, we can quickly calculate and compose S-2A images with differentfeatures at 10 m resolution, as listed in Table 1. The vegetation indices and texture featureswere referred to the supplementary materials of Chen et al. [ 29] and Wang et al. [ 14],respectively. The texture features were derived based on the NNIR band.Table 1. S-2A schemes with different features.Scheme Features Feature Variables1 Spetral bandsBlue (B), green (G), eed (R), red edge 1 (RE1), rededge 2 (RE2), red edge 3 (RE3), near-infrared (NIR),narrow NIR (NNIR), shortwave infrared 1 (SWIR1),shortwave infrared 2 (SWIR2)2Spetral bands, vegetationindices, and textureB, G, R, RE1, RE2 RE3, NIR, NNIR, SWIR1, SWIR2,modiﬁed normalized difference water index(MNDWI), normalized difference build-up index(NDBI), normalized difference vegetation index(NDVI), red-edge NDVI (RENDVI)), sum average(SAVG), correlation (CORR), dissimilarity (DISS)2.3. Sample DataThe sample data were collected using a handheld global positioning system everyyear, and an interpretation mark library was established as a reference for image labeling.We investigated five sample plots (~1 km2) with a total area of 4.41 km2and collected509, 532, and 695 sample points in 2019, 2020, and 2021, respectively. The sampleplots were shown in Figure 2a–d. Then, the training data sets and validation sampledata sets were accomplished based on the interpretation mark library. As noted byLuo et al. [ 30], the selection of representative samples is vital for classification accuracyevaluation. Therefore, six validation sample plots (256 256 pixels) were selected eachyear to evaluate the prediction classification accuracy. In addition, we randomly selected100 sample points (Figure 1) each year to verify the two data sets based on a groundsurvey. The classification verification based on sample plots can reduce the impact of thelocation uncertainty of sample points on the accuracy of evaluation results. The resultsrevealed 13 misclassifications, including NCL and GH, soybean and other crops, GHand urban, and peanuts and other crops (such as alfalfa). The re-correction training datasets shown in Figure 2e–h satisfy the label quality and the training requirements. Thevalidation sample plots were shown in Figure 2i–l.Remote Sens. 2022 ,14, 5216 6 of 19Remote Sens. 2022, 14, x FOR PEER REVIEW   6 of 20    Figure 2. Sample plots, training data set, and validation  sample plots. (a) Sample plot 1 (S1) in 2019; (b) sample plot 2 (S2) in 2020; (c) sample plot 4 (S4) in 2021; (d) sample plot 5 (S5) in 2021; (e) training data set of Site A in 2019; (f) training data set of Site B in 2019; (g) training data set of Site C in 2020; (h) training data set of Site D in 2020; (i) validation  sample plot (VS1) in 2019; (j) validation  sample plot (VS4) in 2020; (k) validation  sample plot (VS3) in 2021; (l) validation  sample plot (VS6) in 2021. 3. Methodology  To evaluate the capability  of the transferable  learning model and the accuracy  of crop segmentation  and classification,  the experiments  in this study were designed  based on the UNet++ architecture.  The workflow  of the experiments  included  image and sample data acquisition  and preprocessing,  model training and assessment,  prediction  result accuracy  evaluation,  and crop mapping  and carbon sequestration.  The modeling  process was per‐formed on an Ubuntu workstation  with 32 GB RAM and one NVIDIA  Tesla T4 graphics  card (16 GB RAM). The training and testing process was implemented  on the Python plat‐form using the PyTorch and GDAL packages.     Figure 2. Sample plots, training data set, and validation sample plots. ( a) Sample plot 1 (S1) in 2019;(b) sample plot 2 (S2) in 2020; ( c) sample plot 4 (S4) in 2021; ( d) sample plot 5 (S5) in 2021; ( e) trainingdata set of Site A in 2019; ( f) training data set of Site B in 2019; ( g) training data set of Site C in 2020;(h) training data set of Site D in 2020; ( i) validation sample plot (VS1) in 2019; ( j) validation sampleplot (VS4) in 2020; ( k) validation sample plot (VS3) in 2021; ( l) validation sample plot (VS6) in 2021.The pixel numbers of training data sets and validation data sets were 43,373,026 and1,180,755, respectively. The pixel proportions of corn, peanuts, soybean, rice, NCL, othercrops, GH, forest land, urban, and water in the training data set were 44.44%, 7.68%, 2.79%,3.34%, 0.69%, 3.45%, 8.48%, 27.64%, 1.16%, and 0.33%, respectively. Figure 2 shows thatcorn and peanuts are the main crops, rice is relatively concentrated, and other crops arescattered. Based on the conclusions of Wang et al. [ 9] and Chen et al. [ 29], the training datasets of sites A, B, C, and D were clipped to 256 256 pixels using the regular-grid clipmethod with a zero-repetition rate. The number of clipped images (in tifformat) and labels(convert from tifformat to pngformat) used for training was 704 pairs.Remote Sens. 2022 ,14, 5216 7 of 193. MethodologyTo evaluate the capability of the transferable learning model and the accuracy of cropsegmentation and classiﬁcation, the experiments in this study were designed based onthe UNet++ architecture. The workﬂow of the experiments included image and sampledata acquisition and preprocessing, model training and assessment, prediction resultaccuracy evaluation, and crop mapping and carbon sequestration. The modeling processwas performed on an Ubuntu workstation with 32 GB RAM and one NVIDIA Tesla T4graphics card (16 GB RAM). The training and testing process was implemented on thePython platform using the PyTorch and GDAL packages.3.1. UNet++ Model and Assessment IndicatorsThe UNet++ architecture was modiﬁed from Zhou et al. [ 21] and shown in Figure 3.The UNet++ used the intermediate convolutional blocks and the skip connections betweenblocks to extract image features and transforms the height and width of the feature mapto the size of the input image. Compared with the UNet model, the re-designed skipconnection could effectively retrieve object details and acquire the shallow and deepfeatures, thus, improving the image segmentation accuracy [ 9,21]. The backbone pre-trained on the ImageNet data set provided parameters to enhance the model performanceand accelerate the convergence on the target task, such as object detection, semanticsegmentation, and ﬁne-grained image recognition, especially in the early training stage [ 31].In this study, we used Timm-RegNetY-320 in the segmentation_models_pytorch library asthe backbone for model training. The RegNetY was proposed by Radosavovic et al. [ 32]and has been veriﬁed to outperform other models, such as the DenseNet, Visual GeometryGroup, and residual networks.Remote Sens. 2022, 14, x FOR PEER REVIEW   7 of 20   3.1. UNet++ Model and Assessment  Indicators  The UNet++ architecture  was modified  from Zhou et al. [21] and shown in Figure 3. The UNet++ used the intermediate  convolutional  blocks and the skip connections  between blocks to extract image features and transforms  the height and width of the feature map to the size of the input image. Compared  with the UNet model, the re‐designed  skip con‐nection could effectively  retrieve object details and acquire the shallow and deep features,  thus, improving  the image segmentation  accuracy  [9,21]. The backbone  pre‐trained on the ImageNet  data set provided  parameters  to enhance the model performance  and accelerate  the convergence  on the target task, such as object detection,  semantic  segmentation,  and fine‐grained image recognition,  especially  in the early training stage [31]. In this study, we used Timm‐RegNetY ‐320 in the segmentation_models_pytorch  library as the backbone  for model training.  The RegNetY  was proposed  by Radosavovic  et al. [32] and has been verified to outperform  other models, such as the DenseNet,  Visual Geometry  Group, and residual networks.   Figure 3. UNet++ architecture  used for crop segmentation  and classification.  Moreover,  three‐fold upsampling  of small samples (including  non‐cultivated  land and water) was used to alleviate the class‐imbalanced  problem and improve the model performance  according  to the pixel proportions  in the training data set. Resulting  from the small number of training‐set pairs, all the pairs were used as training data sets and validation  data sets with different  linear stretching  methods.  The enhancement  strategy of training data included  horizontal  flip, vertical flip, diagonal  flip, and 5% linear stretch; random 0.8%, 1%, or 2% linear stretching  was applied to the validation  data. In addition,  to accelerate  the model convergence  and avoid local minima, the cosine annealing  learn‐ing rate schedule  with restarts was adopted to replace the fixed constant learning rate in the training process [33]. The initial learning rate, weight decay, number of epochs for the first restart (T_0), and value controlling  the speed of the learning rate (T_mult ) were set to 0.0001, 0.001, 2, and 2, respectively.   The model performance  was evaluated  by using the joint loss function and mean in‐tersection  over union (mIoU). The joint loss function involved  the label smoothing  cross entropy loss function and dice coefficient  loss, each with a weight of 0.5. For details of both loss functions,  please refer to the website (https://github.com/BloodAxe/pytorch ‐toolbelt (accessed  on 5 June 2022)). The smoothing  factor of the label smoothing  cross en‐tropy loss function was set to 0.1 to enhance the model generalization  ability. Dice loss was applied in image segmentation  to weaken the effect of imbalanced  class problems.  Figure 3. UNet++ architecture used for crop segmentation and classiﬁcation.Moreover, three-fold upsampling of small samples (including non-cultivated landand water) was used to alleviate the class-imbalanced problem and improve the modelperformance according to the pixel proportions in the training data set. Resulting fromthe small number of training-set pairs, all the pairs were used as training data sets andvalidation data sets with different linear stretching methods. The enhancement strategyof training data included horizontal ﬂip, vertical ﬂip, diagonal ﬂip, and 5% linear stretch;random 0.8%, 1%, or 2% linear stretching was applied to the validation data. In addition,to accelerate the model convergence and avoid local minima, the cosine annealing learningrate schedule with restarts was adopted to replace the ﬁxed constant learning rate in theRemote Sens. 2022 ,14, 5216 8 of 19training process [ 33]. The initial learning rate, weight decay, number of epochs for theﬁrst restart ( T_0), and value controlling the speed of the learning rate ( T_mult ) were set to0.0001, 0.001, 2, and 2, respectively.The model performance was evaluated by using the joint loss function and meanintersection over union (mIoU). The joint loss function involved the label smoothing crossentropy loss function and dice coefﬁcient loss, each with a weight of 0.5. For details of bothloss functions, please refer to the website (https://github.com/BloodAxe/pytorch-toolbelt(accessed on 5 June 2022)). The smoothing factor of the label smoothing cross entropy lossfunction was set to 0.1 to enhance the model generalization ability. Dice loss was appliedin image segmentation to weaken the effect of imbalanced class problems. The mIoU wasused in multi-class tasks to assess the average accuracy between the ground truth andclassiﬁcation of each category.3.2. Baseline Classiﬁcation Models and Evaluation IndicatorsWe selected UNet, DeepLab V3+ [ 34], and the pyramid scene parsing network(PSPNet) [ 35] to compare the model performance and prediction classiﬁcation accuracy. TheUNet architecture appeared to be more conveniently modiﬁed and achieved satisfactorysegmentation results based on few training samples. The DeepLab V3+ extended by FCNshowcased the importance of atrous spatial pyramid pooling for the image segmentationtasks and we added a decoder module to reﬁne the boundary details and improve thesegmentation accuracy. The PSPNet used a spatial pyramid pooling module to harvestdifferent sub-region representations for pixel-level prediction. Here, we use the overallaccuracy (OA), user’s accuracy ( UA), producer’s accuracy ( P A),F1score, and macro F1(Equation (1)) to evaluate the prediction classiﬁcation accuracies. The macro F 1is theaverage value of F1for each category, and is calculated as follows:macro F 1=1NNåi=12UA iPAiUA i+PAi(1)where Ndenotes the number of categories, and UA iand PAiare the user’s accuracy andproducer’s accuracy of the ithcategory, respectively.3.3. Estimation of Crop Carbon SequestrationThe carbon sequestration of the farmland ecosystem mainly includes crop carbonsequestration and soil carbon sequestration. In this study, the carbon sequestration of cropsrefers to the amount of ﬁxed C (carbon) in the total biomass by photosynthesis [ 26]. Dueto the category limitations in complex agricultural areas, corn, soybean, peanut, rice, andother crops were selected to estimate carbon sequestration. Here, the results of other cropswere calculated based on the statistical data of vegetables. Building on previous carbonsequestration works [ 23,36,37], the estimation of crop carbon sequestration in this studyrefers to the carbon of economic yield ( Ce, Equation (2)), straw ( Cs, Equation (2)), and root(Cr, Equation (2)) [ 36]. The function of crop carbon sequestration estimation is shown inEquation (3). The estimation parameters are listed in Table 2 [ 37]. The image segmentationand classiﬁcation results provide the crop area, and the average yield of each crop comesfrom the Henan Provincial People’s Government (http://www.henan.gov.cn/ (accessedon 23 May 2022)) and the Henan Province Bureau of Statistics (https://tjj.henan.gov.cn/(accessed on 5 June 2022)). Currently, it is challenging to obtain actual yield data ofdifferent crops due to the small-holder farming mode in China. Furthermore, accurateyield estimates for some crops, such as peanuts, remain elusive. Previous studies usuallyused statistical yearbook data to estimate crop carbon sequestration. However, due tothe difference between the classiﬁed area of crops in remote sensing images and thestatistical yearbook, the average crop yield was used to reduce crop carbon sequestrationestimation errors. Affected by the lag of the statistical yearbook, some crop yield values,Remote Sens. 2022 ,14, 5216 9 of 19including soybean and rice in 2021, were obtained by averaging over the last four years.Equations (2) and (3) are as follows:Ce=BYA(1 wi),Cs=Ce/L Ce,Cr=(Ce+Cs)R (2)CT=åiCi=åiBiYiAi(1 wi)(1+Ri)Li(3)where CTdenotes the total carbon sequestration of crops in farmland ecosystem, and Ciisthe carbon sequestration of the ithcategory. For the ithcategory, Biis the carbon absorptionrate, Yiis the average yield, Aiis the classiﬁcation area, wiis the water content, Riis theroot-shoot ratio, and Liis the economic coefﬁcient, i.e., the ratio of economic output tobiological output.Table 2. Estimation parameters of crop carbon sequestration.Type Carbon Content ( Bi)/% Water Content ( wi)/% Root-Shoot Ratio ( Ri) Economic Coefﬁcient ( Li)Corn 0.470 0.13 0.170 0.438Peanuts 0.450 0.10 0.200 0.556Soybean 0.450 0.13 0.130 0.425Rice 0.410 0.12 0.125 0.489Other crops 0.450 0.90 0.250 0.8304. Results4.1. Performance Evaluation of the UNet++ ModelFigure 4 shows the model training results based on the UNet++ architecture for bothschemes and upsampling methods. The training process was stopped when the jointloss and mIoU values were not improved relative to the previous 10 consecutive results.The training data set was increased from 704 to 2573 pairs by upsampling of NCL andwater. The total epochs and training time of both schemes were 72 and 72, and 445.27 and450.09 min, respectively. After approximately the 33nd epoch, the result curves for bothschemes became smoother and more stable. The optimal results were achieved in the 62ndepoch, with a joint loss and mIoU of 0.434 and 0.869, and 0.432 and 0.871, respectively.The model performance with upsampling between both schemes was very small, and thedifferences of joint loss values and mIoU values were only 0.002.Remote Sens. 2022, 14, x FOR PEER REVIEW   10 of 20    Figure 4. Training  parameter  results of the two schemes with upsampling  of small samples.  4.2. Evaluation  of Baseline Models and Prediction  Classification  Accuracy  Based on the same training data set, backbone,  scheme 2 (Table 1), and upsampling,  Table 3 shows the training parameter  results of the UNet, DeepLab  V3+, and PSPNet mod‐els. It can be seen that the total epoch of these models is approximately  the same as that of the UNet++ model, while the optimal performance  results are lower than that of the UNet++ model. The joint loss values of UNet, DeepLab  V3+, and PSPNet are 0.067, 0.089, and 0.156 higher than that of the UNet++ model, respectively,  whereas the mIoU values of these models are 0.075, 0.094, and 0.163 lower than that of UNet++ model, respectively.  The training time of UNet and PSPNet is shorter than that of UNet++, but the UNet++ model yielded a better performance.  The advantages  of the modified  UNet++model  were evaluated  mainly from the model performance  and the prediction  classification  accuracy  in this study. Table 3 shows that the modified  UNet++model  achieves  lower joint loss and higher mIoU values compared  with other deep learning models. In addition,  the overall accuracy  and F1 score of the prediction  results were evaluated  to demonstrate  the ad‐vantages  of the modified  UNet++ model. Tables 3 and 4 show that, based on the feature selection  and upsampling  of small samples,  the UNet, Deeplab V3+, and PSPNet models have higher mIoU values and higher prediction  accuracy  than those of the UNet++ model without feature fusion or upsampling  methods.  The PSPNet can complete  the model train‐ing in the shortest time. However,  its prediction  classification  accuracy  is the lowest com‐pared with other models. Therefore,  the improved  UNet++ model yields a better general‐ization capability  and spatiotemporal  transfer performance  than the baseline models, ac‐cording to the model performance  results and prediction  classification  accuracy.  Table 3. Training  parameter  results of different  models. Model  Total Epoch Loss mIoU Total Time Average  Time UNet  72  0.499 0.796  331.43  4.60 DeepLab  V3+  72  0.521 0.777  527.65  7.33 PSPNet  70  0.588 0.708  162.51  2.32 To further clarify the transferable  learning capability  and the influence  on the predic‐tion classification  accuracy,  the OA and macro F1 of different  schemes with or without upsampling  were compared  based on the validation  sample plots and points. The Figure 4. Training parameter results of the two schemes with upsampling of small samples.Remote Sens. 2022 ,14, 5216 10 of 19In addition, we completed the training models of the two schemes without upsamplingto further analyze the inﬂuence on model performance. The total epoch and total time ofschemes 1 and 2 were 136 and 72, and 279.39 and 151.90 min, respectively. At the 126th and62nd epochs, the models obtained the optimal joint loss and mIoU values of 0.492 and 0.795,and 0.489 and 0.798, respectively. The training times without upsampling are shorter thanthose with upsampling because of the unchanged number of training data sets. However,the total epoch of scheme 1 (Table 1) without upsampling was signiﬁcantly higher thanthat with upsampling. The mIoU values of the models without upsampling were about7% lower than those with upsampling. Therefore, the upsampling of small samples caneffectively alleviate the class-imbalanced problem and improve the model performance.The UNet++ models with feature fusion and upsampling yielded higher mIoU and lowerjoint loss values, indicating the applicability in complex crop-classiﬁcation tasks.4.2. Evaluation of Baseline Models and Prediction Classiﬁcation AccuracyBased on the same training data set, backbone, scheme 2 (Table 1), and upsampling,Table 3 shows the training parameter results of the UNet, DeepLab V3+, and PSPNetmodels. It can be seen that the total epoch of these models is approximately the same asthat of the UNet++ model, while the optimal performance results are lower than that of theUNet++ model. The joint loss values of UNet, DeepLab V3+, and PSPNet are 0.067, 0.089,and 0.156 higher than that of the UNet++ model, respectively, whereas the mIoU valuesof these models are 0.075, 0.094, and 0.163 lower than that of UNet++ model, respectively.The training time of UNet and PSPNet is shorter than that of UNet++, but the UNet++model yielded a better performance. The advantages of the modiﬁed UNet++model wereevaluated mainly from the model performance and the prediction classiﬁcation accuracyin this study. Table 3 shows that the modiﬁed UNet++model achieves lower joint lossand higher mIoU values compared with other deep learning models. In addition, theoverall accuracy and F 1score of the prediction results were evaluated to demonstratethe advantages of the modiﬁed UNet++ model. Tables 3 and 4 show that, based on thefeature selection and upsampling of small samples, the UNet, Deeplab V3+, and PSPNetmodels have higher mIoU values and higher prediction accuracy than those of the UNet++model without feature fusion or upsampling methods. The PSPNet can complete themodel training in the shortest time. However, its prediction classiﬁcation accuracy is thelowest compared with other models. Therefore, the improved UNet++ model yields abetter generalization capability and spatiotemporal transfer performance than the baselinemodels, according to the model performance results and prediction classiﬁcation accuracy.Table 3. Training parameter results of different models.Model Total Epoch Loss mIoU Total Time Average TimeUNet 72 0.499 0.796 331.43 4.60DeepLab V3+ 72 0.521 0.777 527.65 7.33PSPNet 70 0.588 0.708 162.51 2.32To further clarify the transferable learning capability and the inﬂuence on the pre-diction classiﬁcation accuracy, the OA and macro F 1of different schemes with or withoutupsampling were compared based on the validation sample plots and points. The predic-tion classiﬁcation accuracies were listed in Table 4. With upsampling of small samples, theOA and macro F 1values of the UNet++ model were higher than those without upsampling.Based on the UNet++ model, the OA and macro F 1differences of scheme 1 with upsamplingin 2019, 2020, and 2021 were 3.88% and 2.17%, 2.16% and 4.11%, and 2.96% and 0.66%higher than those without upsampling, respectively, whereas those values for scheme 2were 7.59% and 7.88%, 6.80% and 6.06%, and 5.51% and 0.09% higher than those withoutupsampling, respectively. In addition to the macro F 1in 2021, the classiﬁcation accuracydifferences UNet++-based between schemes 1 and 2 without upsampling were lower thanthose with upsampling.Remote Sens. 2022 ,14, 5216 11 of 19Table 4. Training parameter results from different schemes and models. Without upsampling andupsampling are abbreviated as WUS and US, respectively.Year IndicatorScheme 1 Scheme 2WUS US WUS US USUNet++ UNet DeepLabV3+ PSPNet2019OA 79.46 83.34 80.59 88.18 86.02 80.26 71.66Macro F 1 50.51 52.68 50.66 58.54 55.11 54.73 47.852020OA 76.15 78.31 80.76 87.56 80.07 81.12 77.99Macro F 1 47.03 51.14 51.29 58.71 54.52 54.73 49.862021OA 76.91 79.87 78.01 83.52 74.76 75.85 67.42Macro F 1 58.43 59.09 59.58 59.67 53.21 52.16 47.39With upsampling of small samples, the prediction classiﬁcation accuracies of PSPNetwere all lower than those of UNet++, UNet, and DeepLab V3+. However, the classiﬁcationaccuracies of UNet and DeepLab V3+ indicated that both models had different advantagesin crop classiﬁcation tasks in complex agricultural planting areas. Therefore, UNet++ isan effective model for classifying multiple categories in complex agricultural areas withrespect to UNet, DeepLab V3+, and PSPNet. We fed the training data set of four sites from2019 to 2020 into different deep learning models with or without sampling and predictedthe classiﬁcation results of the study area in 2019, 2020, and 2021. The prediction accuracieswithout training data sets demonstrated the better transferable learning performance andgeneralization of the UNet++ model.4.3. Reclassiﬁcation Rules and Crop MappingBased on the overall indicators, Table 4 showed that scheme 2 of the UNet++ modelyields the optimal classiﬁcation results. Compared with the results in 2019 and 2020 inTable 4, the OA and macro F 1values of the prediction classiﬁcation results in 2021 werealmost lower than those in the previous two years. In addition to the lack of training datasets, the rainstorm and continuous precipitation since 20 July 2021 have caused seriousdamage to farmland. Compared with the images of 2019 and 2020 in Figure 5, the extremeprecipitation led to notable water stains and ponding on the farmland until 9 September2021. Figure 5c shows that the images of the disaster area were misclassiﬁed as othercrops, urban areas, and water. Thus, it is essential to establish classiﬁcation rules to furtherprocess the misclassiﬁcation results in 2021. The reclassiﬁcation rules processed in ArcMapsoftware are as follows:(1) We manually drew the approximate boundary of the disaster area shown in Figure 6c,and then converted the three-year classiﬁcation results into the vector format;(2) The disaster area was spatially intersected with the 2021 classiﬁcation results (in-cluding other crops, cities, and water) and the result was named as 2021_ disaster_intersect_area ;(3) The 2021_ disaster_ intersect_area data was spatially intersected with the 2019 and 2020classiﬁcation results (including corn, peanuts, soybean, rice, NCL, other crops, andgreenhouses), respectively, and the results were named as 2019_ disaster_ intersect_areaand 2020_ disaster_ intersect_area , respectively;(4) We spatially intersected 2021_ disaster_ intersect_area and 2019_ disaster_ intersect_area ,and 2021_ disaster_ intersect_area and 2020_ disaster_ intersect_area , respectively, andthe results were named as 2019_2021_ disaster_ intersect_area and 2020_2021_ disaster_intersect_area , respectively. Finally, after operating by merge tool, attribute ﬁeldassignment, dissolve tool, and manual editing, the disaster area result was shown inFigure 5d. The land use and crop mapping of each year recorrected by using the roaddata of Xinxiang City were shown in Figure 6.Remote Sens. 2022 ,14, 5216 12 of 19Remote Sens. 2022, 14, x FOR PEER REVIEW   12 of 20   greenhouses),  respectively,  and the results were named as 2019_ disaster_ inter‐sect_area and 2020_ disaster_ intersect_area , respectively;  (4) We spatially  intersected  2021_ disaster_ intersect_area  and 2019_ disaster_ inter‐sect_area , and 2021_ disaster_ intersect_area  and 2020_ disaster_ intersect_area , respec‐tively, and the results were named as 2019_2021_  disaster_ intersect_area  and 2020_2021_  disaster_ intersect_area , respectively.  Finally, after operating  by merge tool, attribute  field assignment,  dissolve tool, and manual editing, the disaster area result was shown in Figure 5d. The land use and crop mapping  of each year recor‐rected by using the road data of Xinxiang  City were shown in Figure 6.  Figure 5. Local segmentation  and classification  results in different  years. (a) Classification  result in 2019; (b) classification  result in 2020; (c,d) classification  and reclassification  results in 2021.  Figure 5. Local segmentation and classiﬁcation results in different years. ( a) Classiﬁcation result in2019; ( b) classiﬁcation result in 2020; ( c,d) classiﬁcation and reclassiﬁcation results in 2021.Remote Sens. 2022, 14, x FOR PEER REVIEW   12 of 20   greenhouses),  respectively,  and the results were named as 2019_ disaster_ inter‐sect_area and 2020_ disaster_ intersect_area , respectively;  (4) We spatially  intersected  2021_ disaster_ intersect_area  and 2019_ disaster_ inter‐sect_area , and 2021_ disaster_ intersect_area  and 2020_ disaster_ intersect_area , respec‐tively, and the results were named as 2019_2021_  disaster_ intersect_area  and 2020_2021_  disaster_ intersect_area , respectively.  Finally, after operating  by merge tool, attribute  field assignment,  dissolve tool, and manual editing, the disaster area result was shown in Figure 5d. The land use and crop mapping  of each year recor‐rected by using the road data of Xinxiang  City were shown in Figure 6.  Figure 5. Local segmentation  and classification  results in different  years. (a) Classification  result in 2019; (b) classification  result in 2020; (c,d) classification  and reclassification  results in 2021.  Figure 6. Segmentation and classiﬁcation results of scheme 2 based on UNet++ model in 2019, 2020,and 2021. ( a) Classiﬁcation result in 2019; ( b) classiﬁcation result in 2020; ( c) classiﬁcation result in2021; ( d) reclassiﬁcation result in 2021.Figure 6d shows that the disaster area in 2021 was mainly distributed in the centralpart (both banks of the Wei River) and the eastern part (along the Yellow River) of XinxiangCity, with the central part being the most severely damaged. Based on the validationsample plots and points, the PA, UA, and F 1score values of the classiﬁcation results inRemote Sens. 2022 ,14, 5216 13 of 192019 and 2020 and the reclassiﬁcation result in 2021 were shown in Table 5. The majorcrops, such as corn, peanuts, soybean, and rice, showed higher classiﬁcation accuracy thanminor crops. Except for the F 1score of soybean in 2021, the F 1scores of corn, peanuts,soybean, and rice were higher than 80%. Due to the few validation samples of NCL, OTH,and GH, their classiﬁcation accuracies were lower. Wang et al. [ 8] pointed out that thefactors, such as within-class diversity, between-class similarity, and mixed pixels, remainmajor challenges in crop and land use classiﬁcation, especially for minor crops. Therefore,it is necessary to further reduce the misclassiﬁcation and omission of small categories byincreasing the number of samples or adopting synthetic minority over-sampling techniquemethods. Since the carbon sequestration was calculated for ﬁve categories in the farmlandecosystem, the accuracies of forest land, urban areas, and water were not further analyzedin this study.Table 5. PA, UA, and F 1score from scheme 2 and the UNet++ model. The abbreviation NaNrepresents not-a-number due to the zero denominator.Year Indicator Corn Peanuts Soybean Rice NCL OTH GH FL Urban Water2019PA 0.94 0.95 0.71 0.97 0.21 0.28 0.00 0.54 0.76 0.57UA 0.89 0.95 0.90 0.95 0.43 0.10 0.00 0.80 0.76 0.31F1score 0.91 0.95 0.80 0.96 0.29 0.15 0.00 0.64 0.76 0.402020UA 0.86 0.95 0.82 0.92 0.08 0.27 NAN 0.76 0.88 0.54PA 0.90 0.79 0.84 0.93 0.59 0.19 NAN 0.91 0.85 0.21F1score 0.88 0.86 0.83 0.93 0.14 0.22 0.00 0.83 0.87 0.312021PA 0.98 0.93 0.83 0.83 0.40 0.15 0.00 0.70 0.76 0.39UA 0.91 0.89 0.75 0.89 0.44 0.17 NAN 0.60 0.71 0.69F1score 0.95 0.91 0.79 0.86 0.42 0.16 0.00 0.65 0.73 0.504.4. Estimation of Crop Area and Carbon SequestrationBased on the classiﬁcation results, Table 6 shows the estimation results of the croparea and carbon sequestration. As a major crop, the area proportion of corn changed littlein recent years. The areas of soybean, rice, and NCL decreased year by year. In 2021, thedisaster area of farmland accounted for about 3.48% of the total study area, and the cropswere mainly corn. Except for the forest land, urban areas, and water, the proportion offarmland in 2019, 2020, and 2021 was 51.44%, 51.64%, and 51.56%, respectively. Here, thecrops in the disaster area were not involved in the carbon sequestration results. For corn,peanuts, soybean, rice, and other crops, the total carbon sequestration in 2019, 2020, and2021 was about 2460.56, 2549.16, and 1814.07 thousand tons, respectively. In 2021, the cropyield was reduced in a large area due to the precipitation disasters, resulting in a signiﬁcantdecrease in crop carbon sequestration.Table 6. Estimation of crop area and carbon sequestration. Area proportion and carbon sequestrationare abbreviated as AP and CS. The units of area and CS area % and thousand tons.Year Indicator Corn Peanuts Soybean Rice NCL OTH GH FL Urban Water2019AP 35.45 7.82 2.19 1.49 1.23 3.17 0.09 14.16 33.14 1.27CS 1997.85 248.13 62.98 64.19 87.412020AP 36.84 6.79 2.15 0.88 0.56 4.31 0.11 13.53 33.18 1.65CS 2114.99 218.43 55.54 40.44 119.762021AP 35.15 7.23 1.35 0.69 0.16 3.33 0.16 12.93 32.88 2.63CS 1442.01 217.09 35.69 32.90 86.38To verify the estimation results of crop carbon sequestration, we compared the resultsto several existing references and methods. The results of Wang et al. [ 38] showed that thecarbon sequestration of corn, peanut, soybean, and rice in Xinxiang City in 2019 and 2020were 2286.63 and 2427.81, 324.40 and 332.12, 59.00 and 43.75, and 116.22 and 76.87 thousandRemote Sens. 2022 ,14, 5216 14 of 19tons, respectively. These estimated results were higher than this assessment owing to thedifferences in crop area and yield in the statistical data. In addition, Tan et al. [ 39] usedthe net carbon sink method to assess the carbon sequestration of cropland ecosystems inHenan Province. The carbon absorption results of corn, peanut, soybean, and rice in XinxiangCity in 2019 and 2020 were 1923.1 and 2041.9, 349.5 and 357.9, 53.9 and 40.0, and 100.8 and66.6 thousand tons, respectively. The estimated results of corn and soybean were lower thanthose of this study, while those of peanut and rice were the opposite. Besides crop yield errors,dry weight ratios used in the method parameters also caused differences in the estimationof crop carbon sequestration. Therefore, it is necessary to further evaluate and analyze theestimation results of different models in combination with the ground survey data.5. Discussion5.1. Evaluation of Model Parameters and Classiﬁcation AccuracyFigure 4 and Table 3 show the inﬂuence of feature fusion and upsampling on the deeplearning model performance with a 256 256 patch size. With upsampling of small samples,the UNet++ model performance of the two schemes showed little difference. However, theUNet++ performance outperformed the UNet, DeepLab V3+, and PSPNet with the sametraining data set and model parameters. Moreover, the segmentation and classiﬁcationresults based on UNet++ also proved its capability for correctly identifying multiplecategories in complex agricultural areas concerning UNet, DeepLab V3+, and PSPNet.As concluded by Zhou et al. [ 21], Wang et al. [ 9], and Giannopoulos et al. [ 18], the majoradvantage of the UNet++ model is the multilevel full-resolution feature map-generatingstrategy for multi-feature RS imagery. In addition, Table 3 showed that the performanceof the UNet model of scheme 2 is better than the DeepLab V3+ model. However, theprediction classiﬁcation accuracy of UNet was not always higher than that of DeepLabV3+. Therefore, when the different deep learning models have similar performances, theprediction classiﬁcation accuracy should be further evaluated.Combined with Figure 6 and Table 5, the prediction accuracies without trainingdata sets demonstrated the generalization of the deep learning models in ground objectmonitoring, especially the UNet++ model. In addition, the cosine annealing learning rateschedule with restarts could further accelerate the model convergence and improve theperformance of deep learning networks. Overall, the image segmentation and classiﬁcationbased on UNet++ was an effective method for classifying the multiple crops referringto UNet, DeepLab V3+, and PSPNet. Due to the inﬂuence of continuous precipitation,the time-series images were difﬁcult to collect and the OA in 2021 was lower than 85%based on single imagery. In current studies, the time-series images and recurrent neuralnetwork (RNN), such as LSTM and transformer, tended to improve the classiﬁcationaccuracy [ 40,41]. Zhong et al. [ 40] developed a deep learning model based on a one-dimensional convolutional network for economic crops using Landsat time-series data,with the highest OA being 85.54% and F 1score of 0.73. Teimouri et al. [ 42] classiﬁed14 major classes of crops based on a convolutional LSTM network, and the OA of 8 majorcrops was more than 84%. Wang et al. [ 41] proposed a novel architecture named the coupledCNN and transformer network and achieved 72.97% mIoU of the Barley Remote SensingDataset. However, the time-series images and deep learning methods would increase thelabeling workload related to ground investigations and manual editing. The experimentalresults of this study demonstrated that the proposed methods can improve the modelperformance and yield state-of-the-art segmentation and classiﬁcation results.The model transferability in this study is mainly shown in the following two points.First, the focus of the current study was placed on the training data set of the four sitesin Xinxiang City, and the training model can be transferred to the segmentation andclassiﬁcation of crops and land use to a more considerable spatial extent. Second, without atraining data set and prior knowledge in 2021, the OA and macro F 1values of predictionclassiﬁcation based on UNet++ model were higher than 83% and 59%, respectively, whichproved the transferable learning capability of the deep learning model across the years.Remote Sens. 2022 ,14, 5216 15 of 19Therefore, the modiﬁed UNet++ model has better spatiotemporal transfer learning abilityin the study area.5.2. Comparison of Local Results and Error AnalysisTable 5 shows that the accuracy is lower for minor categories, such as non-cultivatedland, other crops, greenhouses, and water. Due to the inﬂuence of image spatial resolution,there are a large number of mixed pixels around urban and rural areas. In addition, thenon-cultivated greenhouses are often scattered and small, which often makes them difﬁcultto effectively distinguish from the ﬁeld paths. Figures 5 and 7 show the main misclassiﬁedland use and crop types, including corn misclassiﬁed as rice and forest land (seedling forestin farmland), peanuts misclassiﬁed as corn and soybean, and ﬁeld roads misclassiﬁedas crops. In addition, as a pixel-based classiﬁcation method, the UNet++ model cannotcompletely avoid but can effectively alleviate the “salt and pepper phenomenon” in theclassiﬁcation. In addition, the representative error of samples and training data setsinﬂuences the land use and crop classiﬁcation accuracy. The representative error dependson the sample size, image resolution, label production, and the difference in the entirecategories over the study area [ 9]. The misclassiﬁcation of disaster areas in 2021 was mainlycaused by the representative error of the samples, especially the difference in the entirecategories. This misclassiﬁcation result means that the more representative sample datasets should be added soon in order to improve the generalization and transferable learningcapability of deep learning models.Remote Sens. 2022, 14, x FOR PEER REVIEW   16 of 20    Figure 7. Validation  sample plots and local‐scale prediction  classification  results of Scheme 2 in 2019, 2020, and 2021. 5.3. Analysis of Crop Carbon Sequestration  Figure 8 shows the estimation  results of crop carbon sequestration  per unit area in 2019, 2020, and 2021. Corn and rice have a stronger carbon sequestration  capacity per unit area compared  to peanuts, soybean,  and other crops. The carbon sequestration  capacity of corn, peanuts,  and other crops decreased  due to the continuous  precipitation  and re‐duced production.  In this study, we used the average yield in the statistical  yearbook  to estimate the results. However,  it is unable to display the difference  in crop carbon seques‐tration in spatial distribution.  As it is affected by farming patterns,  it is often difficult to obtain the measured  yield of different  crops. Combining  the RS time‐series vegetation  in‐dices models to obtain a more accurate crop yield can further eliminate  the estimation  error of carbon sequestration  [4]. Moreover,  the carbon sequestration  model in this study does not fully consider  the physiological  parameters  of crops or the performance  of dif‐ferent models, leading to some errors in the estimation  results. Soil carbon sequestration,  as an important  component  of the farmland  ecosystem,  has not been evaluated.  The farm‐land ecosystem  is an important  part of the carbon source/sink,  and an increasing  number of studies show that some measures  should be adopted to improve the carbon sequestra ‐tion capacity,  such as by increasing  the proportion  of the tertiary industry,  optimizing  crop structure,  and increasing  green areas [22,37]. In a word, to accurately  assess crop carbon sequestration  and promote  the process of carbon neutrality,  we need to collect more detailed data and build a rigorous mechanism  model simultaneously.  Figure 7. Validation sample plots and local-scale prediction classiﬁcation results of Scheme 2 in 2019,2020, and 2021.The factors, such as planting patterns of small farmers, diversiﬁed crop types, andtopography and climate difference terrain in large-scale regions, limit the applicability andparameters of the transferable learning models [ 7]. Therefore, the image classiﬁcation in alarge-scale region should be divided into different subregions according to Tobler’s FirstLaw. In this way, the representativeness error of samples and the ﬁne-tuning parameters ofthe model can limit the error propagation within each subregion and improve the modelperformance and prediction accuracy [ 9,12,15]. Therefore, in addition to the feature fusion,training set enhancement, and upsampling of the small samples, the geographical zonesRemote Sens. 2022 ,14, 5216 16 of 19should be involved to reduce the representative error and improve the prediction accuracyat a large scale.5.3. Analysis of Crop Carbon SequestrationFigure 8 shows the estimation results of crop carbon sequestration per unit area in2019, 2020, and 2021. Corn and rice have a stronger carbon sequestration capacity per unitarea compared to peanuts, soybean, and other crops. The carbon sequestration capacity ofcorn, peanuts, and other crops decreased due to the continuous precipitation and reducedproduction. In this study, we used the average yield in the statistical yearbook to estimatethe results. However, it is unable to display the difference in crop carbon sequestrationin spatial distribution. As it is affected by farming patterns, it is often difﬁcult to obtainthe measured yield of different crops. Combining the RS time-series vegetation indicesmodels to obtain a more accurate crop yield can further eliminate the estimation error ofcarbon sequestration [ 4]. Moreover, the carbon sequestration model in this study does notfully consider the physiological parameters of crops or the performance of different models,leading to some errors in the estimation results. Soil carbon sequestration, as an importantcomponent of the farmland ecosystem, has not been evaluated. The farmland ecosystem isan important part of the carbon source/sink, and an increasing number of studies showthat some measures should be adopted to improve the carbon sequestration capacity, suchas by increasing the proportion of the tertiary industry, optimizing crop structure, andincreasing green areas [ 22,37]. In a word, to accurately assess crop carbon sequestrationand promote the process of carbon neutrality, we need to collect more detailed data andbuild a rigorous mechanism model simultaneously.Remote Sens. 2022, 14, x FOR PEER REVIEW   17 of 20    Figure 8. Estimation  results of crop carbon sequestration  per unit area in 2019, 2020, and 2021. 5.4. Limitations  and Future Work Many deep learning approaches  based on domain knowledge  and expertise  have been designed  to segment and classify the land use and crops from RS imagery [29,34]. The variance in geographical  categories  and meteorological  conditions  constrained  the generalizability  and transferability  of semantic  segmentation  methods,  especially  in large‐scale crop mapping  applications  [9]. In the classification  task, many factors, such as dif‐ferences in terrain, crop types and phenology,  and imagery style would affect the transfer learning performance  of the training model. According  to Tobler’s first law of geography,  these limitations  lead to crop segmentation  and classification  models that can only be ap‐plied within a certain geographical  area. Wang et al. [9] noted that it was necessary  to construct  geographical  zones to reduce the representative  error on the transferable  learn‐ing model of crop classification  on a large scale. In addition,  a satisfactory  result can be achieved  by using histogram  matching,  or histogram  specification,  which was utilized to adjust the characteristics  from the different  dates and imaging conditions.  Therefore,  the actual transfer of a deep learning model to a larger spatiotemporal  scale may be challeng‐ing because of different  factors, such as crop types, phenological  period, and upsampling  of small samples.  The experimental  results showed that the upsampling  of small samples improved  the model performance.  However,  the upsampling  times of different  categories  should be fur‐ther analyzed  to alleviate the imbalance  problem,  such as the lower accuracy  of green‐houses. Furthermore,  feature selection  was based on several studies rather than the fea‐ture sensitivity  analysis or importance  evaluation.  The representative  samples caused by extreme weather or other disasters  should be increased  to improve  the generalization  and robustness  capability  of the model. Moreover,  the proposed  model also had shortcomings  on a larger scale. The prediction  results of the UNet++ model with feature fusion and up‐sampling  methods  in the rice‐growing  area of Xinyang City (approximately  4° latitude difference  from Site C) were not satisfactory.  Therefore,  combined  with the digital eleva‐tion model and crop phenological  data, it is necessary  to divide the large region into dif‐ferent subregions  with relatively  consistent  crop planting conditions  to avoid representa ‐tiveness error propagation  and improve the model performance  and classification  accu‐racy.  6. Conclusions  An end‐to‐end transferable  learning model based on the UNet++ architecture  is pro‐posed for crop classification  in complex  agricultural  areas. First, based on the feature fu‐sion and upsampling  of small samples,  the UNet++ model yields the best performance  and classification  accuracy  when compared  to UNet, DeepLab  V3+, and PSPNet, with a Figure 8. Estimation results of crop carbon sequestration per unit area in 2019, 2020, and 2021.5.4. Limitations and Future WorkMany deep learning approaches based on domain knowledge and expertise havebeen designed to segment and classify the land use and crops from RS imagery [ 29,34].The variance in geographical categories and meteorological conditions constrained thegeneralizability and transferability of semantic segmentation methods, especially in large-scale crop mapping applications [ 9]. In the classiﬁcation task, many factors, such asdifferences in terrain, crop types and phenology, and imagery style would affect the transferlearning performance of the training model. According to Tobler’s ﬁrst law of geography,these limitations lead to crop segmentation and classiﬁcation models that can only beapplied within a certain geographical area. Wang et al. [ 9] noted that it was necessaryto construct geographical zones to reduce the representative error on the transferablelearning model of crop classiﬁcation on a large scale. In addition, a satisfactory result canbe achieved by using histogram matching, or histogram speciﬁcation, which was utilizedto adjust the characteristics from the different dates and imaging conditions. Therefore, theRemote Sens. 2022 ,14, 5216 17 of 19actual transfer of a deep learning model to a larger spatiotemporal scale may be challengingbecause of different factors, such as crop types, phenological period, and upsampling ofsmall samples.The experimental results showed that the upsampling of small samples improvedthe model performance. However, the upsampling times of different categories shouldbe further analyzed to alleviate the imbalance problem, such as the lower accuracy ofgreenhouses. Furthermore, feature selection was based on several studies rather than thefeature sensitivity analysis or importance evaluation. The representative samples caused byextreme weather or other disasters should be increased to improve the generalization androbustness capability of the model. Moreover, the proposed model also had shortcomingson a larger scale. The prediction results of the UNet++ model with feature fusion andupsampling methods in the rice-growing area of Xinyang City (approximately 4latitudedifference from Site C) were not satisfactory. Therefore, combined with the digital elevationmodel and crop phenological data, it is necessary to divide the large region into differentsubregions with relatively consistent crop planting conditions to avoid representativenesserror propagation and improve the model performance and classiﬁcation accuracy.6. ConclusionsAn end-to-end transferable learning model based on the UNet++ architecture is pro-posed for crop classiﬁcation in complex agricultural areas. First, based on the featurefusion and upsampling of small samples, the UNet++ model yields the best performanceand classiﬁcation accuracy when compared to UNet, DeepLab V3+, and PSPNet, with alower joint loss value and higher mIoU value of 0.432 and 0.871, respectively. The OA andmacro F 1values of the UNet++ model based on the spatiotemporal transfer experimentare higher than 83% and 58%, respectively. Subsequently, according to the three-year timeseries classiﬁcation results and reclassiﬁcation rules, the disaster area in 2021 accountedfor about 3.48% of the total area and was concentrated in the middle part of the studyarea. Finally, the total carbon sequestration of the ﬁve target crops in 2019, 2020, and2021 was estimated by integrating statistical data, with the values of 2460.56, 2549.16, and1814.07 thousand tons, respectively. These results can provide data and method supportfor damage assessment and carbon sequestration assessment in farmland ecosystems. Theprediction classiﬁcation accuracy without prior knowledge available from ground sam-ples proves that the improved UNet++ model can provide better spatiotemporal transferlearning capability with respect to the baseline models. The transferable learning model issuitable for automatically extracting regional crop information from multi-feature imageryto produce near-real-time crop maps. The experimental conclusion can provide a methodreference to RS segmentation and classiﬁcation, and offer inspiration for understanding thecrop carbon sequestration in farmland ecosystems.Author Contributions: Conceptualization, L.W. and F.Q.; methodology, L.W.; investigation, Y.B. andL.W.; supervision, J.W. and F.Q.; writing—original draft, L.W.; writing—review and editing, L.W.and Z.Z.; resources, C.L.; validation, Y.B. and X.J. All authors have read and agreed to the publishedversion of the manuscript.Funding: This study was supported by the National Natural Science Foundation of China [grant numberU21A2014]; National Science and Technology Platform Construction [grant number 2005DKA32300];Major Research Projects of the Ministry of Education [grant number 16JJD770019]; The Open Programof Collaborative Innovation Center of Geo-Information Technology for Smart Central Plains HenanProvince [grant number G202006]; Key Laboratory of Geospatial Technology for Middle and LowerYellow River Regions (Henan University), Ministry of Education [grant number GTYR202203]; ChinaPostdoctoral Science Foundation [grant number 2018M64099]; Key Scientific Research Projects in Collegesand Universities of Henan Province [grant number 21A420001]; and 2022 Henan College Student’sInnovation and Entrepreneurship Training Program [grant number 202210475034].Data Availability Statement: The code is shared at https://github.com/AgriRS/SummerCrop_Deeplearning.Remote Sens. 2022 ,14, 5216 18 of 19Acknowledgments: We sincerely thank the anonymous reviewers for their constructive commentsand insightful suggestions that greatly improved the quality of this manuscript.Conﬂicts of Interest: The authors declare that they have no known competing ﬁnancial interests orpersonal relationships that could have appeared to inﬂuence the work reported in this paper.References1. Guo, L.; Sun, X.R.; Fu, P .; Shi, T.Z.; Dang, L.N.; Chen, Y.Y.; Linderman, M.; Zhang, G.L.; Zhang, Y.; Jiang, Q.H.; et al. Mappingsoil organic carbon stock by hyperspectral and time-series multispectral remote sensing images in low-relief agricultural areas.Geoderma 2021 ,398, 115118. [CrossRef]2. Anderegg, W.R.L.; Trugman, A.T.; Badgley, G.; Anderson, C.M.; Bartuska, A.; Ciais, P .; Cullenward, D.; Field, C.B.; Freeman,J.; Goetz, S.J.; et al. Climate-driven risks to the climate mitigation potential of forests. Science 2020 ,368, eaaz7005. [CrossRef][PubMed]3. Chapungu, L.; Nhamo, L.; Gatti, R.C. Estimating biomass of savanna grasslands as a proxy of carbon stock using multispectralremote sensing. Remote Sens. Appl. Soc. Environ. 2020 ,17, 100275. [CrossRef]4. Zhao, J.F.; Liu, D.S.; Cao, Y.; Zhang, L.J.; Peng, H.W.; Wang, K.L.; Xie, H.F.; Wang, C.Z. An integrated remote sensing and modelapproach for assessing forest carbon ﬂuxes in China. Sci. Total Environ. 2022 ,811, 152480. [CrossRef] [PubMed]5. Li Johansson, E.; Brogaard, S.; Brodin, L. Envisioning sustainable carbon sequestration in Swedish farmland. Environ. Sci. Policy2022 ,135, 16–25. [CrossRef]6. Xu, J.F.; Yang, J.; Xiong, X.G.; Li, H.F.; Huang, J.F.; Ting, K.C.; Ying, Y.B.; Lin, T. Towards interpreting multi-temporal deep learningmodels in crop mapping. Remote Sens. Environ. 2021 ,264, 112599. [CrossRef]7. Liu, X.K.; Zhai, H.; Shen, Y.L.; Lou, B.K.; Jiang, C.M.; Li, T.Q.; Hussain, S.B.; Shen, G.L. Large-scale crop mapping from multisourceremote sensing images in Google Earth Engine. IEEE J.-Stars 2020 ,13, 414–427. [CrossRef]8. Wang, L.J.; Wang, J.Y.; Qin, F. Feature fusion approach for temporal land use mapping in complex agricultural areas. Remote Sens.2021 ,13, 2517. [CrossRef]9. Wang, L.J.; Wang, J.Y.; Zhang, X.W.; Wang, L.G.; Qin, F. Deep segmentation and classiﬁcation of complex crops using multi-featuresatellite imagery. Comput. Electron. Agric. 2022 ,200, 107249. [CrossRef]10. Camps-Valls, G.; Bioucas-Dias, J.; Crawford, M. A special issue on advances in machine learning for remote sensing andgeosciences. IEEE Geosci. Remote Sens. Mag. 2016 ,4, 5–7. [CrossRef]11. Watts, J.D.; Lawrence, R.L.; Miller, P .R.; Montagne, C. Monitoring of cropland practices for carbon sequestration purposes innorth central Montana by Landsat remote sensing. Remote Sens. Environ. 2009 ,113, 1843–1852. [CrossRef]12. Reichstein, M.; Camps-Valls, G.; Stevens, B.; Jung, M.; Denzler, J.; Carvalhais, N.; Prabhat. Deep learning and process understand-ing for data-driven Earth system science. Nature 2019 ,566, 195–204. [CrossRef]13. Morid, M.A.; Borjali, A.; Del Fiol, G. A scoping review of transfer learning research on medical image analysis using ImageNet.Comput. Biol. Med. 2021 ,128, 104115. [CrossRef]14. Wang, L.J.; Wang, J.Y.; Liu, Z.Z.; Zhu, J.; Qin, F. Evaluation of a deep-learning model for multispectral remote sensing of land useand crop classiﬁcation. Crop J. 2022 ,10, 1435–1451. [CrossRef]15. Yu, R.G.; Fu, X.Z.; Jiang, H.; Wang, C.H.; Li, X.W.; Zhao, M.K.; Ying, X.; Shen, H.Q. Remote sensing image segmentation bycombining feature enhanced with fully convolutional network. Lect. Notes Comput. Sci. 2018 ,11301 , 406–415.16. Ma, L.; Liu, Y.; Zhang, X.L.; Ye, Y.X.; Yin, G.F.; Johnson, B.A. Deep learning in remote sensing applications: A meta-analysis andreview. Isprs J. Photogramm. 2019 ,152, 166–177. [CrossRef]17. Li, J.T.; Shen, Y.L.; Yang, C. An adversarial generative network for crop classiﬁcation from remote sensing timeseries images.Remote Sens. 2021 ,13, 65. [CrossRef]18. Giannopoulos, M.; Tsagkatakis, G.; Tsakalides, P . 4D U-Nets for multi-temporal remote sensing data classiﬁcation. Remote Sens.2022 ,14, 634. [CrossRef]19. Yang, L.B.; Huang, R.; Huang, J.F.; Lin, T.; Wang, L.M.; Mijiti, R.; Wei, P .L.; Tang, C.; Shao, J.; Li, Q.Z.; et al. Semantic segmentationbased on temporal features: Learning of temporal-spatial information from time-series SAR images for paddy rice mapping.IEEE Trans. Geosci. Remote 2022 ,60, 4403216. [CrossRef]20. Wang, C.S.; Du, P .F.; Wu, H.R.; Li, J.X.; Zhao, C.J.; Zhu, H.J. A cucumber leaf disease severity classiﬁcation method based on thefusion of DeepLabV3+and U-Net. Comput. Electron. Agric. 2021 ,189, 106373. [CrossRef]21. Zhou, Z.W.; Siddiquee, M.M.R.; Tajbakhsh, N.; Liang, J.M. UNet plus plus: A nested U-Net architecture for medical imagesegmentation. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Dlmia 2018 ; Springer:Cham, Switzerland, 2018; Volume 11045, pp. 3–11.22. Zhang, A.X.; Deng, R.R. Spatial-temporal evolution and inﬂuencing factors of net carbon sink efﬁciency in Chinese cities underthe background of carbon neutrality. J. Clean. Prod. 2022 ,365, 132547. [CrossRef]23. Tang, X.L.; Zhao, X.; Bai, Y.F.; Tang, Z.Y.; Wang, W.T.; Zhao, Y.C.; Wan, H.W.; Xie, Z.Q.; Shi, X.Z.; Wu, B.F.; et al. Carbon pools inChina’s terrestrial ecosystems: New estimates based on an intensive ﬁeld survey. Proc. Natl. Acad. Sci. USA 2018 ,115, 4021–4026.[CrossRef]Remote Sens. 2022 ,14, 5216 19 of 1924. Pineux, N.; Lisein, J.; Swerts, G.; Bielders, C.L.; Lejeune, P .; Colinet, G.; Degre, A. Can DEM time series produced by UAV be usedto quantify diffuse erosion in an agricultural watershed? Geomorphology 2017 ,280, 122–136. [CrossRef]25. Feyisa, G.L.; Palao, L.K.; Nelson, A.; Gumma, M.K.; Paliwal, A.; Win, K.T.; Nge, K.H.; Johnson, D.E. Characterizing and mappingcropping patterns in a complex agro-ecosystem: An iterative participatory mapping procedure using machine learning algorithmsand MODIS vegetation indices. Comput. Electron. Agric. 2020 ,175, 105595. [CrossRef]26. Chen, R.; Zhang, R.Y.; Han, H.Y.; Jiang, Z.D. Is farmers’ agricultural production a carbon sink or source?—Variable systemboundary and household survey data. J. Clean. Prod. 2020 ,266, 122108. [CrossRef]27. Zhang, Y.L.; Song, C.H.; Hwang, T.; Novick, K.; Coulston, J.W.; Vose, J.; Dannenberg, M.P .; Hakkenberg, C.R.; Mao, J.F.; Woodcock,C.E. Land cover change-induced decline in terrestrial gross primary production over the conterminous United States from 2001to 2016. Agric. For. Meteorol. 2021 ,308, 108609. [CrossRef]28. Wang, Y.C.; Tao, F.L.; Yin, L.C.; Chen, Y. Spatiotemporal changes in greenhouse gas emissions and soil organic carbon sequestrationfor major cropping systems across China and their drivers over the past two decades. Sci. Total Environ. 2022 ,833, 155087.[CrossRef]29. Chen, Z.Y.; Li, D.L.; Fan, W.T.; Guan, H.Y.; Wang, C.; Li, J. Self-attention in reconstruction bias U-Net for semantic segmentationof building rooftops in optical remote sensing images. Remote Sens. 2021 ,13, 2524. [CrossRef]30. Luo, B.H.; Yang, J.; Song, S.L.; Shi, S.; Gong, W.; Wang, A.; Du, L. Target classiﬁcation of similar spatial characteristics in complexurban areas by using multispectral LiDAR. Remote Sens. 2022 ,14, 238. [CrossRef]31. Ienco, D.; Interdonato, R.; Gaetano, R.; Minh, D.H.T. Combining Sentinel-1 and Sentinel-2 satellite image time series for landcover mapping via a multi-source deep learning architecture. Isprs J. Photogramm. 2019 ,158, 11–22. [CrossRef]32. Radosavovic, I.; Johnson, J.; Xie, S.N.; Lo, W.Y.; Dollar, P . On network design spaces for visual recognition. In Proceedings of theIEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea, 27 October–2 November 2019; pp. 1882–1890.33. Jung, H.; Choo, C. SGDR: A simple GPS-based disrupt-tolerant routing for vehicular networks. In Proceedings of the 2017International Conference on Information and Communication Technology Convergence (ICTC), Jeju Island, Korea, 18–20 October2017; pp. 1013–1016.34. Chen, L.C.E.; Zhu, Y.K.; Papandreou, G.; Schroff, F.; Adam, H. Encoder-decoder with atrous separable convolution for semanticimage segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), Munich, Germany, 8–14 September2018; Volume 11211, pp. 833–851.35. Zhao, H.S.; Shi, J.P .; Qi, X.J.; Wang, X.G.; Jia, J.Y. Pyramid scene parsing network. In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, Honolulu, HI, USA, 21–26 July 2017; pp. 6230–6239.36. Shi, L.G.; Fan, S.C.; Kong, F.L.; Chen, F. Preliminary study on the carbon efﬁciency of main crops production in North China Plain.Acta Agron. Sin. 2011 ,37, 1485–1490. [CrossRef]37. Zhang, P .Y.; He, J.J.; Pang, B.; Lu, C.P .; Qin, M.Z.; Lu, Q.C. Temporal and spatial differences in carbon footprint in farmlandecosystem: A case study of Henan Province, China. Chin. J. Appl. Ecol. 2017 ,28, 3050–3060.38. Wang, L.; Liu, Y.Y.; Zhang, Y.H.; Dong, S.H. Spatial and temporal distribution of carbon source/sink and decomposition ofinﬂuencing factors in farmland ecosystem in Henan Province. Acta Sci. Circumstantiae 2022 , 1–13. [CrossRef]39. Tan, M.Q.; Cui, Y.P .; Ma, X.Z.; Liu, P .; Fan, L.; Lu, Y.Y.; Wen, W.; Chen, Z. Study on carbon sequestration estimation of croplandecosystem in Henan Province. J. Ecol. Rural. Environ. 2022 ,9, 1–14.40. Zhong, L.H.; Hu, L.N.; Zhou, H. Deep learning based multi-temporal crop classiﬁcation. Remote Sens. Environ. 2019 ,221, 430–443.[CrossRef]41. Wang, H.; Chen, X.Z.; Zhang, T.X.; Xu, Z.Y.; Li, J.Y. CCTNet: Coupled CNN and Transformer Network for crop segmentation ofremote sensing images. Remote Sens. 2022 ,14, 1956. [CrossRef]42. Teimouri, N.; Dyrmann, M.; Jorgensen, R.N. A novel spatio-temporal FCN-LSTM network for recognizing various crop typesusing multi-temporal radar images. Remote Sens. 2019 ,11, 990. [CrossRef]